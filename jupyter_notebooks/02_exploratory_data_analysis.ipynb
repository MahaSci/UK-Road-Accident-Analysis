{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Conduct statistical testing to explore and understand the nature of the dataset in terms of its distribution and tendency, idenfity anomalies present, and get a general overview of the data's patterns.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* Write down which data or information you need to run the notebook \n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Write here which files, code or artefacts you generate by the end of the notebook \n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* If you have any additional comments that don't fit in the previous bullets, please state them here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libaries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before conducting statistical and graphical analysis, we must first inspect the dataset itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "df = pd.read_csv(\"filtered_accident_data_set.csv\") \n",
    "\n",
    "# Display basic structure \n",
    "print(\"Dataset Shape:\", df.shape)  # Provides the number of rows and columns in the dataset\n",
    "print(\"First few rows:\\n\", df.head()) # Displays the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cleaned and filtered dataset amounts to 31,494 rows, with 13 columns. Using the head() function, we can see the first few rows. However, these functions alone do not tell us about the completeness of the dataset or whether the data types have been correctly set. Therefore, we will use the Pandas `info()` function to get a better look at the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of datatypes, non-null's and memory usage\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this output, the following is evident:\n",
    "- The majority of the columns (9 out of 13) are categorical, as indicated by the `object` data type.\n",
    "- None of the columns contain null values which makes it ideal for analysis.\n",
    "- The Accident Date column is incorrectly set as an `object` type. This will need to be converted to a datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Accident Date' column to datetime format\n",
    "df['Accident Date'] = pd.to_datetime(df['Accident Date'], dayfirst=True)\n",
    "\n",
    "# dayfirst=True explicitly specifies the UK date format (day/month/year). Default is month/day/year.\n",
    "\n",
    "df['Accident Date'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the column is now set to a datetime format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the recorded date is correct and in the UK format, we can extract the constituent parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_check = df['Accident Date'].iloc[0] # Get the first date in the column\n",
    "print(date_to_check) # Display the date in the column\n",
    "\n",
    "# Extract the day, month and year from the date to verify the conversion in UK date format\n",
    "df['Accident Date'].iloc[0].strftime('%d %B %Y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate (Non-Graphical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate analysis is focused on inspecting one given variable at a time. Its main purpose is to serve as a summary of the data column's central tendency and the level of variation within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics (numerical)\n",
    "- describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Count**: As expected, the counts remain identical, indicating that there are no missing values within the dataset.\n",
    "  \n",
    "- **Mean**: \n",
    "    - **Number of Casualties**: The mean casualty count is **1.38**, suggesting that each accident results in just over 1 casualty on average.\n",
    "    - **Number of Vehicles**: The mean vehicle count is **1.85**, implying that each accident involves around 1-2 vehicles.\n",
    "\n",
    "- **Min**:\n",
    "    - **Number of Casualties**: The minimum number of casualties in any accident is **1**.\n",
    "    - **Number of Vehicles**: The minimum number of vehicles involved in an accident is **1**.\n",
    "\n",
    "- **Max**:\n",
    "    - **Number of Casualties**: The maximum number of casualties in a single accident is **24**.\n",
    "    - **Number of Vehicles**: The maximum number of vehicles involved in an accident is **10**.\n",
    "\n",
    "- **Standard Deviation**:\n",
    "    - **Number of Casualties**: The standard deviation is **0.82**, indicating that while most accidents involve 1 or 2 casualties, there are a few with significantly more.\n",
    "    - **Number of Vehicles**: The standard deviation is **0.67**, suggesting that most accidents involve around 1-2 vehicles, with a few accidents involving more.\n",
    "\n",
    "- **Percentiles**:\n",
    "    - **25% (Q1)**: Most accidents (25% of the dataset) involve **1 casualty** and **1 vehicle**.\n",
    "    - **50% (Median)**: The median accident involves **1 casualty** and between **1-2 vehicles**.\n",
    "    - **75% (Q3)**: 75% of accidents involve **1 casualty** and **2 vehicles**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median & Mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the mean was covered in the describe() function, we will calculate the mode and median.\n",
    "\n",
    "- Mode: to identify the most frequent value in the dataset columns.\n",
    "- Median: to provide a 'typical' value of a given column in the dataset that is resistant to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **mode** (most frequent values) in the dataset provides insights into the most common characteristics of accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode of the columns\n",
    "df.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Accident Severity**: Slight\n",
    "- **Accident Date**: 2020-07-09\n",
    "- **Latitude**: 52.458798\n",
    "- **Light Conditions**: Daylight\n",
    "- **District Area**: Birmingham\n",
    "- **Longitude**: -1.871043\n",
    "- **Number of Casualties**: 1\n",
    "- **Number of Vehicles**: 2\n",
    "- **Road Surface Conditions**: Dry\n",
    "- **Road Type**: Single carriageway\n",
    "- **Urban or Rural Area**: Urban\n",
    "- **Vehicle Type**: Car\n",
    "\n",
    "This row represents the most frequent combination of attributes in the dataset, giving an overview of the typical accident scenario based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median for numerical values only\n",
    "df.select_dtypes(include='number').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **median** values for the numerical columns in the dataset represent the central point (50th percentile) of the data. Below are the median values for the relevant numerical attributes:\n",
    "\n",
    "- **Latitude**: 52.481789 (central latitude of accident locations)\n",
    "- **Longitude**: -1.901860 (central longitude of accident locations)\n",
    "- **Number of Casualties**: 1 (median number of casualties in accidents)\n",
    "- **Number of Vehicles**: 2 (median number of vehicles involved in accidents)\n",
    "\n",
    "These values indicate typical or central tendencies for each of the numerical columns in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness, Kurtosis, and Shapiro-Wilk Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ignoring **Longitude** and **Latitude** because they represent geographic positions, not quantities that follow a distribution we would typically assess using these statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test skewness of numerical columns\n",
    "df.select_dtypes(include='number').skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Number of Casualties and Number of Vehicles show positive skewness, suggesting that most accidents tend to involve fewer vehicles and casualties, with a few outliers having more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test kurtosis of numerical columns\n",
    "df.select_dtypes(include='number').kurt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis measures the \"tailedness\" of a distribution. A high kurtosis indicates that the data has heavy tails or outliers, while low kurtosis suggests lighter tails and fewer outliers.\n",
    "\n",
    "Number of Casualties shows a high kurtosis, meaning there are some accidents with significantly more casualties than the majority.\n",
    "Number of Vehicles also has moderate kurtosis, suggesting that most accidents involve fewer vehicles, but there are some accidents with notably more vehicles involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro-Wilk test for normality\n",
    "from scipy import stats\n",
    "\n",
    "shapiro = stats.shapiro(df.select_dtypes(include='number'))\n",
    "shapiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test result shows that the data is not normally distributed:\n",
    "\n",
    "- **Statistic: 0.5952:** A value significantly lower than 1 indicates a substantial deviation from normality.\n",
    "- **p-value:** The extremely small p-value suggests strong evidence against the null hypothesis, which states that the data is normally distributed. Since the p-value is far below the common significance level of 0.05, we reject the null hypothesis and conclude that the data does not follow a normal distribution.\n",
    "\n",
    "This means that, moving forward, we will need to use non-parametric methods for statistical testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate (Non-Graphical)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
